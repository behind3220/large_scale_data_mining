{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hR8xaHRoLDu2"
   },
   "source": [
    "## Module Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBo-LU4YNCgj",
    "outputId": "29427864-25a5-45e1-c704-5fed25fcabc5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import string\n",
    "from string import punctuation\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.metrics import auc, roc_curve, plot_roc_curve, plot_confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn import datasets, metrics, model_selection, svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tempfile import mkdtemp\n",
    "from joblib import Memory\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cluster import KMeans,  AgglomerativeClustering\n",
    "from sklearn.metrics.cluster import contingency_matrix, homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from scipy.optimize import linear_sum_assignment\n",
    "!pip install umap-learn\n",
    "import umap\n",
    "!pip install hdbscan\n",
    "import hdbscan\n",
    "import pickle\n",
    "import bz2\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plotmat import plot_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sZdk9wafgtJ",
    "outputId": "181720dd-14e1-4951-e7d7-638fdc43b7c8"
   },
   "outputs": [],
   "source": [
    "# helper code\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRBt3Y1TM8Fm"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86v0MTa1UwHa"
   },
   "source": [
    "Report the dimensions of the TF-IDF matrix you obtain.\n",
    "\n",
    "Ans:\n",
    "\n",
    "the dimensions of the TF-IDF matrix is (7882, 23522)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-u513VgNB3-",
    "outputId": "0fb8b0ab-e889-4679-bf8b-8e2ea920310c"
   },
   "outputs": [],
   "source": [
    "categories = ['comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware',\n",
    "              'comp.sys.mac.hardware','rec.autos','rec.motorcycles','rec.sport.baseball','rec.sport.hockey']\n",
    "dataset = fetch_20newsgroups(subset = 'all', categories = categories, shuffle = True, random_state = 0,remove=('headers','footers'))\n",
    "\n",
    "vec = CountVectorizer(stop_words='english', min_df=3)\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "X_vec = vec.fit_transform(dataset.data)\n",
    "X_tfidf = tfidf.fit_transform(X_vec)\n",
    "print(\"TFIDX dataset shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBvgFDEEVBKn"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXeuwwNDVKZ8"
   },
   "source": [
    "Report the contingency table of your clustering result. You may use\n",
    "the provided plotmat.py to visualize the matrix. Does the contingency matrix have to be square-shaped?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAbpR1y-yAV7"
   },
   "source": [
    "Ans:\n",
    "\n",
    "The contingency table is \n",
    "\n",
    "|3232|671|\n",
    "|---|---|\n",
    "|54|3925|\n",
    "\n",
    "The contingency matrix doesn't have to be square-shaped but it's usually square-shaped. It has normally same rows and columns because the reference data and the map should have same dimensions (categories). However, it's not necessary, a category can exist in the reference data but doesn't exist in the map, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKiuIzCQU9Re"
   },
   "outputs": [],
   "source": [
    "# map_root = {\"comp.graphics\":0, \"comp.os.ms-windows.misc\":0, \"comp.sys.ibm.pc.hardware\":0, \"comp.sys.mac.hardware\":0,\n",
    "#             \"rec.autos\":1, \"rec.motorcycles\":1, \"rec.sport.baseball\":1, \"rec.sport.hockey\":1}\n",
    "# y_data = dataset.target.map(map_root)\n",
    "y_data = []\n",
    "for i in dataset.target:\n",
    "  if i < 4:\n",
    "    y_data.append(0)\n",
    "  else:\n",
    "    y_data.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2qi0Lplcg30",
    "outputId": "17315106-0ef7-414e-c976-7c8a5e8a813e"
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2, init='k-means++', max_iter=2000, n_init=50, random_state=0)\n",
    "km.fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "wqkgNnZZgFnE",
    "outputId": "02b759f0-6755-4c5f-e173-753a65281f14"
   },
   "outputs": [],
   "source": [
    "plot_mat(contingency_matrix(y_data, km.labels_), size=(8, 6), xticklabels=['comp.','rec.'], yticklabels=['comp.','rec.'], pic_fname='Q2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7LXZQVVVEO3"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5JEa3RdWNgl"
   },
   "source": [
    "Report the 5 clustering measures explained in the introduction for Kmeans clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irnFZJ2xhh1-"
   },
   "source": [
    "Ans:\n",
    "\n",
    "- Homogeneity: 0.5999\n",
    "- Completeness: 0.6121\n",
    "- V-measure: 0.6059\n",
    "- Adjusted Rand-Index: 0.6659\n",
    "- Adjusted Mutual Information Score: 0.6059"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifXih7O1etPJ",
    "outputId": "987f3cce-c1bc-4742-8139-cb72a100329d"
   },
   "outputs": [],
   "source": [
    "def print_5_measure_scores(y_data, labels):\n",
    "  print(\"Homogeneity: %0.4f\" % homogeneity_score(y_data, labels))\n",
    "  print(\"Completeness: %0.4f\" % completeness_score(y_data, labels))\n",
    "  print(\"V-measure: %0.4f\" % v_measure_score(y_data, labels))\n",
    "  print(\"Adjusted Rand-Index: %0.4f\"% adjusted_rand_score(y_data, labels))\n",
    "  print(\"Adjusted Mutual Information Score: %0.4f\"% adjusted_mutual_info_score(y_data, labels))\n",
    "print_5_measure_scores(y_data, km.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4jctkYQh7hf"
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DH4ISvIIXA53"
   },
   "source": [
    "Report the plot of the percentage of variance that the top r principle\n",
    "components retain v.s. r, for r = 1 to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "KkjW8zKzaI4R",
    "outputId": "39a98191-ba90-4c77-888a-10c4389b54f3"
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=1000, random_state=0)\n",
    "X_LSI = svd.fit_transform(X_tfidf)\n",
    "\n",
    "ratios = np.cumsum(svd.explained_variance_ratio_)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(np.linspace(1, 1000, 1000), ratios, lw=2, linestyle='--')\n",
    "\n",
    "ax.set_ylabel('cumulative sum of explained_variance_ratio')\n",
    "ax.set_xlabel('principal components (r)')\n",
    "plt.title('the percentage of variance that the top r principle components retain v.s. r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffzjf7Smbs-v"
   },
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EO01x5JDeZ2F"
   },
   "source": [
    "Let $r$ be the dimension that we want to reduce the data to (i.e. n components).\n",
    "Try $r$ = 1, 10, 20, 50, 100, 300, and plot the 5 measure scores v.s. $r$ for both SVD and NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZ5rHFnfejrd",
    "outputId": "54ef7d67-0ace-4608-b2ce-7c7655d4cb53"
   },
   "outputs": [],
   "source": [
    "rs5 = [1, 10, 20, 50, 100, 300]\n",
    "# svd\n",
    "svd_homo = []\n",
    "svd_comp = []\n",
    "svd_vmes = []\n",
    "svd_ranI = []\n",
    "svd_muts = []\n",
    "# nmf\n",
    "nmf_homo = []\n",
    "nmf_comp = []\n",
    "nmf_vmes = []\n",
    "nmf_ranI = []\n",
    "nmf_muts = []\n",
    "kmm = KMeans(n_clusters=2, init='k-means++', max_iter=2000, n_init=50, random_state=0)\n",
    "for i in range(len(rs5)):\n",
    "  print(\"Try r =\", rs5[i], \"...\")\n",
    "  # SVD\n",
    "  svd_tmp = TruncatedSVD(n_components=rs5[i], random_state=0)\n",
    "  X_svd = svd_tmp.fit_transform(X_tfidf)\n",
    "  km_svd = kmm.fit(X_svd)\n",
    "  svd_homo.append(homogeneity_score(y_data, km_svd.labels_))\n",
    "  svd_comp.append(completeness_score(y_data, km_svd.labels_))\n",
    "  svd_vmes.append(v_measure_score(y_data, km_svd.labels_))\n",
    "  svd_ranI.append(adjusted_rand_score(y_data, km_svd.labels_))\n",
    "  svd_muts.append(adjusted_mutual_info_score(y_data, km_svd.labels_))\n",
    "  # NMF\n",
    "  nmf_tmp = NMF(n_components=rs5[i], init='random', random_state=0)\n",
    "  X_nmf = nmf_tmp.fit_transform(X_tfidf)\n",
    "  km_nmf = kmm.fit(X_nmf)\n",
    "  nmf_homo.append(homogeneity_score(y_data, km_nmf.labels_))\n",
    "  nmf_comp.append(completeness_score(y_data, km_nmf.labels_))\n",
    "  nmf_vmes.append(v_measure_score(y_data, km_nmf.labels_))\n",
    "  nmf_ranI.append(adjusted_rand_score(y_data, km_nmf.labels_))\n",
    "  nmf_muts.append(adjusted_mutual_info_score(y_data, km_nmf.labels_))\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OO_0mZ3TtIKx"
   },
   "outputs": [],
   "source": [
    "def print_bar_scores_result(width, rs, svd_score, nmf_score, score_name, title):\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  # ax.plot(rs, svd_homo, color='r', label=\"SVD\")\n",
    "  # ax.plot(rs, nmf_homo, color='b', label=\"NMF\")\n",
    "  ax.bar(np.arange(len(rs)) - width/2, svd_score, width, label=\"SVD\")\n",
    "  ax.bar(np.arange(len(rs)) + width/2, nmf_score, width, label=\"NMF\")\n",
    "  ax.set_xticks(np.arange(len(rs)))\n",
    "  ax.set_xticklabels(rs)\n",
    "\n",
    "  ax.legend()\n",
    "  ax.set_ylabel(score_name)\n",
    "  ax.set_xlabel('r')\n",
    "  plt.title(title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iU0g87IIkMvm",
    "outputId": "78461a1d-fca2-407c-ea03-568edbfe7088"
   },
   "outputs": [],
   "source": [
    "print_bar_scores_result(0.3, rs5, svd_homo, nmf_homo, 'Homogeneity', 'Homogenity Score for different r [SVD vs. NMF]')\n",
    "print_bar_scores_result(0.3, rs5, svd_comp, nmf_comp, 'Completeness', 'Completeness Score for different r [SVD vs. NMF]')\n",
    "print_bar_scores_result(0.3, rs5, svd_vmes, nmf_vmes, 'V-measure', 'V-measure Score for different r [SVD vs. NMF]')\n",
    "print_bar_scores_result(0.3, rs5, svd_ranI, nmf_ranI, 'Adjusted Rand-Index', 'Adjusted Rand-Index for different r [SVD vs. NMF]')\n",
    "print_bar_scores_result(0.3, rs5, svd_muts, nmf_muts, 'Adjusted Mutual Information Score', 'Adjusted Mutual Information Score for different r [SVD vs. NMF]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDOVHP4u79YG"
   },
   "source": [
    "Report a good choice of $r$ for SVD and NMF respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5zCdBb2zu2N"
   },
   "source": [
    "Ans:\n",
    "\n",
    "From the graph above, we can find that a good choice of $r$ for SVD could be $r = 50$, a good choice of $r$ for NMF could be $r = 10$.\n",
    "\n",
    "Because we can find that there are no significant increase for SVD after $r = 50$. And as $r$ increases, we need to realize that the Euclidean distances in K-means we use will converge to a constant value, such that K-means doesn't perform well in high-dimensional prediction. Considering to this, choosing $r = 50$ is good enough. On the other hand, NMF performs obviously best when $r = 10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujin9GQr0cWK"
   },
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-XBp_5e0geU"
   },
   "source": [
    "How do you explain the non-monotonic behavior of the measures as $r$\n",
    "increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L33ZwYs_0lAg"
   },
   "source": [
    "Ans:\n",
    "\n",
    "As $r$ increases, the performance of the measures doesn't increase correspondingly. It's because that as $r$ increases, it indicates that the target matrix behind it will become complicated and high-dimensional. In which restrained the performance of K-means as we mentioned before. The Euclidean distances in K-means will converge to a constant value between the sample points. The clustering method can't find centroids for different sample points so that its performance becomes poor.\n",
    "\n",
    "On the other hand, we can find that SVD maintain the similar score performance even after $r$ keeps increasing, however, NMF doesn't perform well after $r$ is larger than $10$. It might because of the fact that SVD is a much more deterministic method than NMF. As we discussed in Project 1, SVD is a more insightful method and is able to interpret high-dimensional data rather than NMF did. Since NMF only uses the positive entries in the reduced matrix and makes assumption about the missing values. SVD doesn't assume anything about the value. Therefore, it's less restricted and performs better in high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9ssQiUz0n2x"
   },
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHcS0k8p0qg2"
   },
   "source": [
    "Are these measures on average better than those computed in Question\n",
    "3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ok6DQVdu0y7d"
   },
   "source": [
    "Ans:\n",
    "\n",
    "No, even though we minus the outlier data ($r = 1$), the average performance of these measures is still worse than those computed in Question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DManEK5pwZuH",
    "outputId": "ef649f8b-531a-4ee7-afca-e5e19937a6f2"
   },
   "outputs": [],
   "source": [
    "print(\"Measure scores computed in Question 3:\")\n",
    "print_5_measure_scores(y_data, km.labels_)\n",
    "\n",
    "print(\"\\nAverage measure scores for SVD:\")\n",
    "print(\"Homogeneity: %0.4f\" % (sum(svd_homo) / len(svd_homo)))\n",
    "print(\"Completeness: %0.4f\" % (sum(svd_comp) / len(svd_comp)))\n",
    "print(\"V-measure: %0.4f\" % (sum(svd_vmes) / len(svd_vmes)))\n",
    "print(\"Adjusted Rand-Index: %0.4f\" % (sum(svd_ranI) / len(svd_ranI)))\n",
    "print(\"Adjusted Mutual Information Score: %0.4f\" % (sum(svd_muts) / len(svd_muts)))\n",
    "\n",
    "print(\"\\nAverage measure scores for NMF:\")\n",
    "print(\"Homogeneity: %0.4f\" % (sum(nmf_homo) / len(nmf_homo)))\n",
    "print(\"Completeness: %0.4f\" % (sum(nmf_comp) / len(nmf_comp)))\n",
    "print(\"V-measure: %0.4f\" % (sum(nmf_vmes) / len(nmf_vmes)))\n",
    "print(\"Adjusted Rand-Index: %0.4f\" % (sum(nmf_ranI) / len(nmf_ranI)))\n",
    "print(\"Adjusted Mutual Information Score: %0.4f\" % (sum(nmf_muts) / len(nmf_muts)))\n",
    "\n",
    "print(\"\\nAverage measure scores for SVD (not including r=1):\")\n",
    "print(\"Homogeneity: %0.4f\" % ( (sum(svd_homo) - svd_homo[0]) / (len(svd_homo) - 1)))\n",
    "print(\"Completeness: %0.4f\" % ( (sum(svd_comp) - svd_comp[0]) / (len(svd_comp) - 1)))\n",
    "print(\"V-measure: %0.4f\" % ( (sum(svd_vmes) - svd_vmes[0]) / (len(svd_vmes) - 1)))\n",
    "print(\"Adjusted Rand-Index: %0.4f\" % ( (sum(svd_ranI) - svd_ranI[0]) / (len(svd_ranI) - 1)))\n",
    "print(\"Adjusted Mutual Information Score: %0.4f\" % ( (sum(svd_muts) - svd_ranI[0]) / (len(svd_muts) - 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eigQ6IC2IVS"
   },
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDcBvVun2K9L"
   },
   "source": [
    "Visualize the clustering results for:\n",
    "\n",
    "- SVD with your optimal choice of r for K-Means clustering;\n",
    "\n",
    "- NMF with your choice of r for K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jty7XNKw2UrP"
   },
   "outputs": [],
   "source": [
    "# best parameter of SVD and NMF\n",
    "best_r_SVD = 50\n",
    "best_r_NMF = 10\n",
    "kmm = KMeans(n_clusters=2, init='k-means++', max_iter=2000, n_init=50, random_state=0)\n",
    "# best SVD\n",
    "best_svd = TruncatedSVD(n_components=best_r_SVD, random_state=0)\n",
    "best_X_svd = best_svd.fit_transform(X_tfidf)\n",
    "y_pred_best_svd = kmm.fit_predict(best_X_svd)\n",
    "# best NMF\n",
    "best_nmf = NMF(n_components=best_r_NMF, init='random', random_state=0)\n",
    "best_X_nmf = best_nmf.fit_transform(X_tfidf)\n",
    "y_pred_best_nmf = kmm.fit_predict(best_X_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "C9nJo0tyQgDe",
    "outputId": "92b05fb2-1cd6-448a-a78f-03424209cefd"
   },
   "outputs": [],
   "source": [
    "plt.scatter(best_X_svd[:,0], best_X_svd[:,1], c=y_data)\n",
    "plt.title(\"SVD with r=50 for clustering with real labels\")\n",
    "plt.show()\n",
    "plt.scatter(best_X_svd[:,0], best_X_svd[:,1], c=y_pred_best_svd)\n",
    "plt.title(\"SVD with r=50 for clustering with K-means predicted labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "k7B3nH1wTk5S",
    "outputId": "240bcc31-ddb1-4a5c-cb71-98118a45e6b1"
   },
   "outputs": [],
   "source": [
    "plt.scatter(best_X_nmf[:,0], best_X_nmf[:,1], c=y_data)\n",
    "plt.title(\"NMF with r=10 for clustering with real labels\")\n",
    "plt.show()\n",
    "plt.scatter(best_X_nmf[:,0], best_X_nmf[:,1], c=y_pred_best_nmf)\n",
    "plt.title(\"NMF with r=10 for clustering with K-means predicted labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYiI8lc02XjS"
   },
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxQAHUnl2Z7V"
   },
   "source": [
    "What do you observe in the visualization? How are the data points of the\n",
    "two classes distributed? Is distribution of the data ideal for K-Means clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laejVqbK2f0d"
   },
   "source": [
    "Ans:\n",
    "\n",
    "1. From the visualization, we can see that there are no spherical distributions on the plots for both SVD or NMF. there are many overlapping points among the two clusters which makes the boundary of two clusters hard to define.\n",
    "2. The data points in the SVD are distributed more ideally than NMF did. Its shape also looks more like a sphere comparing to NMF one. On the other hand, we can analyze the performances from their homogeneity scores. SVD gets a higher score than NMF did but it's still not great enough.\n",
    "3. In conclusion, the distribution of the data is not ideal for K-Means clustering. K-Means clustering assumes spherical shapes of clusters but we can not find a similar shape no matter in SVD or NMF here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUgpaJ2B2g_R"
   },
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaG_hjmI2noA"
   },
   "source": [
    "Load documents with the same configuration as in Question 1, but for\n",
    "ALL 20 categories.\n",
    "\n",
    "\n",
    "There is a mismatch between cluster labels and class labels. For example, the cluster #3 may correspond to the class #8. As a result, the high-value entries of the 20 × 20 contingency matrix can be scattered around, making it messy to inspect, even if the clustering result is not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4bQAV4827yK",
    "outputId": "4a8243b6-0ed6-421b-bbb9-4b6cca14393e"
   },
   "outputs": [],
   "source": [
    "dataset_all = fetch_20newsgroups(subset = 'all', shuffle = True, random_state = 0,remove=('headers','footers'))\n",
    "\n",
    "vec = CountVectorizer(stop_words='english', min_df=3)\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "X_a_vec = vec.fit_transform(dataset_all.data)\n",
    "X_a_tfidf = tfidf.fit_transform(X_a_vec)\n",
    "print(\"TFIDX dataset shape:\", X_a_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cm9KJJw4gArW"
   },
   "outputs": [],
   "source": [
    "y_a_data = dataset_all.target\n",
    "\n",
    "kma = KMeans(n_clusters=20, init='k-means++', max_iter=2000, n_init=50, random_state=0)\n",
    "# kma.fit(X_a_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKawXijmzK_2"
   },
   "source": [
    "Construct the TF-IDF matrix, reduce its dimensionality using BOTH NMF\n",
    "and SVD (specify settings you choose and why).\n",
    "\n",
    "Ans:\n",
    "\n",
    "We choose $r = 20$ for SVD, $r = 10$ for NMF respectively. we will explain the reason in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOA5vBHkg10f",
    "outputId": "5f3c2c48-a83a-416c-e1a0-2c8e85df9fb4"
   },
   "outputs": [],
   "source": [
    "rs10 = [1, 5, 10, 20, 50, 100]\n",
    "# svd\n",
    "a_svd_homo = []\n",
    "a_svd_comp = []\n",
    "a_svd_vmes = []\n",
    "a_svd_ranI = []\n",
    "a_svd_muts = []\n",
    "# nmf\n",
    "a_nmf_homo = []\n",
    "a_nmf_comp = []\n",
    "a_nmf_vmes = []\n",
    "a_nmf_ranI = []\n",
    "a_nmf_muts = []\n",
    "for i in range(len(rs10)):\n",
    "  print(\"Try r =\", rs10[i], \"...\")\n",
    "  # SVD\n",
    "  svd_tmp = TruncatedSVD(n_components=rs10[i], random_state=0)\n",
    "  X_a_svd = svd_tmp.fit_transform(X_a_tfidf)\n",
    "  kma_svd = kma.fit(X_a_svd)\n",
    "  a_svd_homo.append(homogeneity_score(y_a_data, kma_svd.labels_))\n",
    "  a_svd_comp.append(completeness_score(y_a_data, kma_svd.labels_))\n",
    "  a_svd_vmes.append(v_measure_score(y_a_data, kma_svd.labels_))\n",
    "  a_svd_ranI.append(adjusted_rand_score(y_a_data, kma_svd.labels_))\n",
    "  a_svd_muts.append(adjusted_mutual_info_score(y_a_data, kma_svd.labels_))\n",
    "  # NMF\n",
    "  nmf_tmp = NMF(n_components=rs10[i], init='random', random_state=0)\n",
    "  X_a_nmf = nmf_tmp.fit_transform(X_a_tfidf)\n",
    "  kma_nmf = kma.fit(X_a_nmf)\n",
    "  a_nmf_homo.append(homogeneity_score(y_a_data, kma_nmf.labels_))\n",
    "  a_nmf_comp.append(completeness_score(y_a_data, kma_nmf.labels_))\n",
    "  a_nmf_vmes.append(v_measure_score(y_a_data, kma_nmf.labels_))\n",
    "  a_nmf_ranI.append(adjusted_rand_score(y_a_data, kma_nmf.labels_))\n",
    "  a_nmf_muts.append(adjusted_mutual_info_score(y_a_data, kma_nmf.labels_))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wZ3trJzchNMy",
    "outputId": "647a634d-2dbf-4dcc-df9f-339e60d10543"
   },
   "outputs": [],
   "source": [
    "print_bar_scores_result(0.3, rs10, a_svd_homo, a_nmf_homo, 'Homogeneity', 'Homogenity Score for different r [SVD vs. NMF]')\n",
    "print_bar_scores_result(0.3, rs10, a_svd_comp, a_nmf_comp, 'Completeness', 'Completeness Score for different r [SVD vs. NMF]')\n",
    "print_bar_scores_result(0.3, rs10, a_svd_vmes, a_nmf_vmes, 'V-measure', 'V-measure Score for different r [SVD vs. NMF]')\n",
    "print_bar_scores_result(0.3, rs10, a_svd_ranI, a_nmf_ranI, 'Adjusted Rand-Index', 'Adjusted Rand-Index for different r [SVD vs. NMF]')\n",
    "print_bar_scores_result(0.3, rs10, a_svd_muts, a_nmf_muts, 'Adjusted Mutual Information Score', 'Adjusted Mutual Information Score for different r [SVD vs. NMF]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "USpGuM57lR1h",
    "outputId": "3a0487b4-f4e4-4dd3-ee11-6ce23c1080d8"
   },
   "outputs": [],
   "source": [
    "avg_svd_homo = sum(a_svd_homo) / len(a_svd_homo)\n",
    "avg_svd_comp = sum(a_svd_comp) / len(a_svd_comp)\n",
    "avg_svd_vmes = sum(a_svd_vmes) / len(a_svd_vmes)\n",
    "avg_svd_ranI = sum(a_svd_ranI) / len(a_svd_ranI)\n",
    "avg_svd_muts = sum(a_svd_muts) / len(a_svd_muts)\n",
    "\n",
    "avg_nmf_homo = sum(a_nmf_homo) / len(a_nmf_homo)\n",
    "avg_nmf_comp = sum(a_nmf_comp) / len(a_nmf_comp)\n",
    "avg_nmf_vmes = sum(a_nmf_vmes) / len(a_nmf_vmes)\n",
    "avg_nmf_ranI = sum(a_nmf_ranI) / len(a_nmf_ranI)\n",
    "avg_nmf_muts = sum(a_nmf_muts) / len(a_nmf_muts)\n",
    "\n",
    "avg_svd = []\n",
    "avg_nmf = []\n",
    "for i in range(len(rs10)):\n",
    "  tmp_svd = 0\n",
    "  tmp_svd += a_svd_homo[i] / avg_svd_homo\n",
    "  tmp_svd += a_svd_comp[i] / avg_svd_comp\n",
    "  tmp_svd += a_svd_vmes[i] / avg_svd_vmes\n",
    "  tmp_svd += a_svd_ranI[i] / avg_svd_ranI\n",
    "  tmp_svd += a_svd_muts[i] / avg_svd_muts\n",
    "  avg_svd.append(tmp_svd / 5)\n",
    "  tmp_nmf = 0\n",
    "  tmp_nmf += a_nmf_homo[i] / avg_nmf_homo\n",
    "  tmp_nmf += a_nmf_comp[i] / avg_nmf_comp\n",
    "  tmp_nmf += a_nmf_vmes[i] / avg_nmf_vmes\n",
    "  tmp_nmf += a_nmf_ranI[i] / avg_nmf_ranI\n",
    "  tmp_nmf += a_nmf_muts[i] / avg_nmf_muts\n",
    "  avg_nmf.append(tmp_nmf / 5)\n",
    "for i in range(len(avg_svd)):\n",
    "  print(\"Average normalized scores for SVD when r =\", rs10[i], \":\", avg_svd[i])\n",
    "print(\"\")\n",
    "for i in range(len(avg_nmf)):\n",
    "  print(\"Average normalized scores for NMF when r =\", rs10[i], \":\", avg_nmf[i])\n",
    "\n",
    "sorted_avg_svd = sorted(avg_svd, reverse=True)\n",
    "sorted_avg_nmf = sorted(avg_nmf, reverse=True)\n",
    "\n",
    "top3_svd = [avg_svd.index(v) for v in sorted_avg_svd[:3]]\n",
    "top3_nmf = [avg_nmf.index(v) for v in sorted_avg_nmf[:3]]\n",
    "\n",
    "print(\"\\nTop 3 best values of r for SVD:\")\n",
    "for i in top3_svd:\n",
    "  print(\"r =\", rs10[i], \":\", avg_svd[i])\n",
    "\n",
    "print(\"\\nTop 3 best values of r for NMF:\")\n",
    "for i in top3_nmf:\n",
    "  print(\"r =\", rs10[i], \":\", avg_nmf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8lXayWiyNyn"
   },
   "source": [
    "We computed the 5 measure scores, normalized it to sum up and get average. As you can see, we obtained the top 3 best values of r for SVD and NMF respectively.\n",
    "\n",
    "Based on above result, we choose $r = 20$ for SVD, $r = 10$ for NMF as setting. \n",
    "\n",
    "As $r$ increases to 100 for SVD, there is no corresponding significant increasing in the normalized scores for it. Therefore, choosing $r = 20$ is good enough.\n",
    "\n",
    "Choosing $r = 10$ for NMF is easy here, since it's lower than $20$ but get a higher normalized average score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY-EhspwzS7k"
   },
   "source": [
    "Visualize the contingency matrix and report the five clustering metrics (DO BOTH\n",
    "NMF AND SVD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "79mPI_IX1EFN",
    "outputId": "15f7260e-71fe-4c4e-a6db-ac80c610575d"
   },
   "outputs": [],
   "source": [
    "best_a_svd = 20\n",
    "# SVD\n",
    "best_svd = TruncatedSVD(n_components=best_a_svd, random_state=0)\n",
    "best_X_a_svd = best_svd.fit_transform(X_a_tfidf)\n",
    "best_kma_svd = kma.fit(best_X_a_svd)\n",
    "cm = confusion_matrix(dataset_all.target, kma.labels_)\n",
    "rows, cols = linear_sum_assignment(cm, maximize=True)\n",
    "plot_mat(cm[rows[:, np.newaxis], cols], xticklabels=cols, yticklabels=rows, title='SVD with best r (r = 20)', size=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6wDog7mD-sE7",
    "outputId": "cc090c36-0874-44be-90fa-c423775edcb5"
   },
   "outputs": [],
   "source": [
    "best_a_nmf = 10\n",
    "# NMF\n",
    "best_nmf = NMF(n_components=best_a_nmf, random_state=0)\n",
    "best_X_a_nmf = best_nmf.fit_transform(X_a_tfidf)\n",
    "best_kma_nmf = kma.fit(best_X_a_nmf)\n",
    "cm = confusion_matrix(dataset_all.target, kma.labels_)\n",
    "rows, cols = linear_sum_assignment(cm, maximize=True)\n",
    "plot_mat(cm[rows[:, np.newaxis], cols], xticklabels=cols, yticklabels=rows, title='NMF with best r (r = 10)', size=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Uj4znt03ByH"
   },
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RF0A8ccK3DzQ"
   },
   "source": [
    "Reduce the dimension of your dataset with UMAP. Consider the following\n",
    "settings: n components = [5, 20, 200], metric = ”cosine” vs. ”euclidean”. If ”cosine” metric fails, please look at the FAQ at the end of this spec.\n",
    "\n",
    "Report the permuted contingency matrix and the five clustering evaluation metrics for the different combinations (6 combinations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JT47ZcKv3Qcf",
    "outputId": "b4ab5424-8ddb-4f3a-d2f2-ed34fdce142a"
   },
   "outputs": [],
   "source": [
    "rs11 = [5, 20, 200]\n",
    "# metrics = ['cosine', 'euclidean']\n",
    "# UMAP cosine\n",
    "umap_cos_homo = []\n",
    "umap_cos_comp = []\n",
    "umap_cos_vmes = []\n",
    "umap_cos_ranI = []\n",
    "umap_cos_muts = []\n",
    "# UMAP euclidean\n",
    "umap_euc_homo = []\n",
    "umap_euc_comp = []\n",
    "umap_euc_vmes = []\n",
    "umap_euc_ranI = []\n",
    "umap_euc_muts = []\n",
    "\n",
    "kma = KMeans(n_clusters=20, init='k-means++', max_iter=2000, n_init=50, random_state=0)\n",
    "\n",
    "for i in range(len(rs11)):\n",
    "  print(\"\\nTry r =\", rs11[i], \"...\")\n",
    "  # cosine\n",
    "  print(\"Cosine UMAP when r =\", rs11[i], \":\")\n",
    "  tmp_umap_cos = umap.UMAP(n_components=rs11[i], metric='cosine')\n",
    "  X_umap_cos = tmp_umap_cos.fit_transform(X_a_tfidf)\n",
    "  kma_cos = kma.fit(X_umap_cos)\n",
    "  umap_cos_homo.append(homogeneity_score(y_a_data, kma_cos.labels_))\n",
    "  umap_cos_comp.append(completeness_score(y_a_data, kma_cos.labels_))\n",
    "  umap_cos_vmes.append(v_measure_score(y_a_data, kma_cos.labels_))\n",
    "  umap_cos_ranI.append(adjusted_rand_score(y_a_data, kma_cos.labels_))\n",
    "  umap_cos_muts.append(adjusted_mutual_info_score(y_a_data, kma_cos.labels_))\n",
    "  # print permuted contingency matrix\n",
    "  cm = confusion_matrix(y_a_data, kma.labels_)\n",
    "  rows, cols = linear_sum_assignment(cm, maximize=True)\n",
    "  plot_mat(cm[rows[:, np.newaxis], cols], xticklabels=cols, yticklabels=rows, title='cosine UMAP with r=' + str(rs11[i]), size=(15,15))\n",
    "  # print the five clustering evaluation metrics\n",
    "  print_5_measure_scores(y_a_data, kma_cos.labels_)\n",
    "  # euclidean\n",
    "  print(\"\\nEuclidean UMAP when r =\", rs11[i], \":\")\n",
    "  tmp_umap_euc = umap.UMAP(n_components=rs11[i], metric='euclidean')\n",
    "  X_umap_euc = tmp_umap_cos.fit_transform(X_a_tfidf)\n",
    "  kma_euc = kma.fit(X_umap_euc)\n",
    "  umap_euc_homo.append(homogeneity_score(y_a_data, kma_euc.labels_))\n",
    "  umap_euc_comp.append(completeness_score(y_a_data, kma_euc.labels_))\n",
    "  umap_euc_vmes.append(v_measure_score(y_a_data, kma_euc.labels_))\n",
    "  umap_euc_ranI.append(adjusted_rand_score(y_a_data, kma_euc.labels_))\n",
    "  umap_euc_muts.append(adjusted_mutual_info_score(y_a_data, kma_euc.labels_))\n",
    "  # print permuted contingency matrix\n",
    "  cm = confusion_matrix(y_a_data, kma.labels_)\n",
    "  rows, cols = linear_sum_assignment(cm, maximize=True)\n",
    "  plot_mat(cm[rows[:, np.newaxis], cols], xticklabels=cols, yticklabels=rows, title='euclidean UMAP with r=' + str(rs11[i]), size=(15,15))\n",
    "  # print the five clustering evaluation metrics\n",
    "  print_5_measure_scores(y_a_data, kma_euc.labels_)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMEZH20k3UAO"
   },
   "source": [
    "## Question 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqMPn9qS3Wm7"
   },
   "source": [
    "Analyze the contingency matrices. Which setting works best and why?\n",
    "\n",
    "Ans:\n",
    "\n",
    "From the contingency metrices we found above, we can see that there are no many differences between the euclidean UMAP with different $r$. Therefore, setting $r = 20$ is good enough for euclidean UMAP.\n",
    "\n",
    "As of cosine UMAP, setting $r = 20$ gives us a better result. Setting $r$ equals to other numbers all gives more outliers or categories being distributed to other categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUxDOlYy3hB3",
    "outputId": "e345766f-de95-404a-f01d-851750f7ee06"
   },
   "outputs": [],
   "source": [
    "avg_cos = []\n",
    "avg_euc = []\n",
    "for i in range(len(rs11)):\n",
    "  tmp_cos = 0\n",
    "  tmp_cos += umap_cos_homo[i] / (sum(umap_cos_homo) / len(umap_cos_homo))\n",
    "  tmp_cos += umap_cos_comp[i] / (sum(umap_cos_comp) / len(umap_cos_comp))\n",
    "  tmp_cos += umap_cos_vmes[i] / (sum(umap_cos_vmes) / len(umap_cos_vmes))\n",
    "  tmp_cos += umap_cos_ranI[i] / (sum(umap_cos_ranI) / len(umap_cos_ranI))\n",
    "  tmp_cos += umap_cos_muts[i] / (sum(umap_cos_muts) / len(umap_cos_muts))\n",
    "  avg_cos.append(tmp_cos / 5)\n",
    "  tmp_euc = 0\n",
    "  tmp_euc += umap_euc_homo[i] / (sum(umap_euc_homo) / len(umap_euc_homo))\n",
    "  tmp_euc += umap_euc_comp[i] / (sum(umap_euc_comp) / len(umap_euc_comp))\n",
    "  tmp_euc += umap_euc_vmes[i] / (sum(umap_euc_vmes) / len(umap_euc_vmes))\n",
    "  tmp_euc += umap_euc_ranI[i] / (sum(umap_euc_ranI) / len(umap_euc_ranI))\n",
    "  tmp_euc += umap_euc_muts[i] / (sum(umap_euc_muts) / len(umap_euc_muts))\n",
    "  avg_euc.append(tmp_euc / 5)\n",
    "for i in range(len(avg_cos)):\n",
    "  print(\"Average normalized scores for cosine UMAP when r =\", rs11[i], \":\", avg_cos[i])\n",
    "print(\"\")\n",
    "for i in range(len(avg_euc)):\n",
    "  print(\"Average normalized scores for euclidean UMAP when r =\", rs11[i], \":\", avg_euc[i])\n",
    "\n",
    "best_r_cos = rs11[avg_cos.index(max(avg_cos))]\n",
    "# best_r_cos = 5\n",
    "best_r_euc = rs11[avg_euc.index(max(avg_euc))]\n",
    "print(\"\\nBest value of r for cosine UMAP:\", best_r_cos, \", its average normalized scores:\", avg_cos[rs11.index(best_r_cos)])\n",
    "print(\"Best value of r for euclidean UMAP:\", best_r_euc, \", its average normalized scores:\", avg_euc[rs11.index(best_r_euc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pA0f_eAJ_bg8"
   },
   "source": [
    "What about for each metric choice?\n",
    "\n",
    "Ans:\n",
    "\n",
    "From the result above, we calculated the average normalized scores for each combination.\n",
    "\n",
    " It also indicates that there are no many differences between the euclidean UMAP with different $r$. Although the scores when $r = 200$ is a little bit better, choosing $r = 20$ is good enough considering to the calculation time.\n",
    "\n",
    "On the other hand, a good choice of $r$ for cosine UMAP is $20$.\n",
    "\n",
    "Comparing the scores of these two UMAP, cosine UMAP with $r = 20$ works best and we choose it for the following comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDED06CB3mTh"
   },
   "source": [
    "## Question 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEOLoYQO3pLv"
   },
   "source": [
    "So far, we have attempted K-Means clustering with 4 different representation\n",
    "learning techniques (sparse TF-IDF representation, PCA-reduced, NMF-reduced, UMAP-reduced).\n",
    "\n",
    "Compare and contrast the clustering results across the 4 choices, and suggest an approach that is best for the K-Means clustering task on the 20-class text data. Choose any choice of clustering metrics for your comparison.\n",
    "\n",
    "Ans:\n",
    "\n",
    "Comparin the clustering results across the 4 choices, we can see that the performance of UMAP-reduced is better than other 3 results. Therefore, we suggest **UMAP-reduced (cosine UMAP with $r = 20$)** is best for the K-Means clustering task on the 20-class text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1D7SHpNFwj4"
   },
   "outputs": [],
   "source": [
    "def print_scores(title_name, score_homo, score_comp, score_vmes, score_ranI, score_muts):\n",
    "  print(title_name)\n",
    "  print(\"Homogeneity: %0.4f\" % score_homo)\n",
    "  print(\"Completeness: %0.4f\" % score_comp)\n",
    "  print(\"V-measure: %0.4f\" % score_vmes)\n",
    "  print(\"Adjusted Rand-Index: %0.4f\" % score_ranI)\n",
    "  print(\"Adjusted Mutual Information Score: %0.4f \\n\" % score_muts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBL5WOAgpAx9"
   },
   "outputs": [],
   "source": [
    "kmm_o = KMeans(n_clusters=20, init='k-means++', max_iter=2000, n_init=50, random_state=0)\n",
    "kmm_o.fit(X_a_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99270vXg3uF6",
    "outputId": "5c1daac0-1962-4bb8-b500-72cd4f0e1202"
   },
   "outputs": [],
   "source": [
    "print(\"scores for sparse TF-IDF representation:\")\n",
    "print_5_measure_scores(y_a_data, kmm_o.labels_)\n",
    "print(\"\")\n",
    "br_svd_i = rs10.index(best_a_svd)\n",
    "br_nmf_i = rs10.index(best_a_nmf)\n",
    "br_cos_i = avg_cos.index(max(avg_cos))\n",
    "print_scores(\"scores for SVD when r = \" + str(best_a_svd) + \":\", a_svd_homo[br_svd_i], a_svd_comp[br_svd_i], a_svd_vmes[br_svd_i], a_svd_ranI[br_svd_i], a_svd_muts[br_svd_i])\n",
    "print_scores(\"scores for NMF when r = \" + str(best_a_nmf) + \":\", a_nmf_homo[br_nmf_i], a_nmf_comp[br_nmf_i], a_nmf_vmes[br_nmf_i], a_nmf_ranI[br_nmf_i], a_nmf_muts[br_nmf_i])\n",
    "print_scores(\"scores for cosine UMAP when r = \" + str(rs11[br_cos_i]) + \":\", umap_cos_homo[br_cos_i], umap_cos_comp[br_cos_i], umap_cos_vmes[br_cos_i], umap_cos_ranI[br_cos_i], umap_euc_muts[br_cos_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwLoxjK030CL"
   },
   "source": [
    "## Question 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9t115Xf32DV"
   },
   "source": [
    "Use UMAP to reduce the dimensionality properly, and perform Agglomerative clustering with n_clusters=20 . Compare the performance of “ward” and “single”\n",
    "linkage criteria.\n",
    "\n",
    "Report the five clustering evaluation metrics for each case.\n",
    "\n",
    "Ans:\n",
    "\n",
    "From the five clustering evaluation metrics indicate below, we can find that the performance of Agglomerative Clustering with single linkage criterion is much worse than ward linkage criterion one did. \n",
    "\n",
    "It might because of the using method behind these two criterions. The ward linkage criterion minimizes the variance of the clusters being merged. The single linkage criterion minimizes the distance between all observations of the two sets. Therefore, single linkage criterion is not robust enough to handle this high dimensional problem. It fails to deal with the noise or outlier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "huSt8i-p35cV",
    "outputId": "c32192f2-4c50-492f-ae77-f9782b152d1d"
   },
   "outputs": [],
   "source": [
    "best_umap = umap.UMAP(n_components=rs11[br_cos_i], metric='cosine')\n",
    "X_umap = best_umap.fit_transform(X_a_tfidf)\n",
    "agg_w = AgglomerativeClustering(n_clusters=20, linkage='ward').fit(X_umap)\n",
    "agg_s = AgglomerativeClustering(n_clusters=20, linkage='single').fit(X_umap)\n",
    "\n",
    "print(\"Agglomerative Clustering with ward linkage criterion:\")\n",
    "print_5_measure_scores(y_a_data, agg_w.labels_)\n",
    "print(\"\\nAgglomerative Clustering with single linkage criterion:\")\n",
    "print_5_measure_scores(y_a_data, agg_s.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRlqbXBU39zC"
   },
   "source": [
    "## Question 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByAoWXle3_6R"
   },
   "source": [
    "Apply HDBSCAN on UMAP-transformed 20-category data.\n",
    "\n",
    "Use min_cluster_size=100 .\n",
    "\n",
    "Vary the min_cluster_size among 20, 100, 200 and report your findings in terms of the five clustering evaluation metrics - you will plot the best contingency matrix in the next question. Feel free to try modifying other parameters in HDBSCAN to get better performance.\n",
    "\n",
    "Ans:\n",
    "\n",
    "From the five clustering evaluation metrics we found, the best min_cluster_size to use here is $100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQ4Ty9Os6oWT",
    "outputId": "b6d30768-05aa-4bd7-9ed0-84bca44c0f61"
   },
   "outputs": [],
   "source": [
    "cluster_sizes = [20, 100, 200]\n",
    "hdbs_homo = []\n",
    "hdbs_comp = []\n",
    "hdbs_vmes = []\n",
    "hdbs_ranI = []\n",
    "hdbs_muts = []\n",
    "\n",
    "for min_size in cluster_sizes:\n",
    "  print(\"\\nmin_cluster_size =\", min_size, \":\")\n",
    "  y_pred_hdbs = hdbscan.HDBSCAN(min_cluster_size=min_size).fit_predict(X_umap)\n",
    "  hdbs_homo.append(homogeneity_score(y_a_data, y_pred_hdbs))\n",
    "  hdbs_comp.append(completeness_score(y_a_data, y_pred_hdbs))\n",
    "  hdbs_vmes.append(v_measure_score(y_a_data, y_pred_hdbs))\n",
    "  hdbs_ranI.append(adjusted_rand_score(y_a_data, y_pred_hdbs))\n",
    "  hdbs_muts.append(adjusted_mutual_info_score(y_a_data, y_pred_hdbs))\n",
    "  print_5_measure_scores(y_a_data, y_pred_hdbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imY4YhUJA5nd",
    "outputId": "4f1c0901-52ac-4ab0-e1c1-e8d8d6cd1e1f"
   },
   "outputs": [],
   "source": [
    "avg_hdbs = []\n",
    "for i in range(len(cluster_sizes)):\n",
    "  tmp_hdbs = 0\n",
    "  tmp_hdbs += hdbs_homo[i] / (sum(hdbs_homo) / len(hdbs_homo))\n",
    "  tmp_hdbs += hdbs_comp[i] / (sum(hdbs_comp) / len(hdbs_comp))\n",
    "  tmp_hdbs += hdbs_vmes[i] / (sum(hdbs_vmes) / len(hdbs_vmes))\n",
    "  tmp_hdbs += hdbs_ranI[i] / (sum(hdbs_ranI) / len(hdbs_ranI))\n",
    "  tmp_hdbs += hdbs_muts[i] / (sum(hdbs_muts) / len(hdbs_muts))\n",
    "  avg_hdbs.append(tmp_hdbs / 5)\n",
    "for i in range(len(avg_hdbs)):\n",
    "  print(\"Average normalized scores for HDBSCAN when min_cluster_size =\", cluster_sizes[i], \":\", avg_hdbs[i])\n",
    "\n",
    "best_mcs = cluster_sizes[avg_hdbs.index(max(avg_hdbs))]\n",
    "print(\"\\nBest value of min_cluster_size for HDBSCAN:\", best_mcs, \", its average normalized scores:\", max(avg_hdbs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ArWg-TM4F00"
   },
   "source": [
    "## Question 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pmmPHQV4JcQ"
   },
   "source": [
    "Contingency matrix\n",
    "\n",
    "Plot the contingency matrix for the best clustering model from Question 15.\n",
    "How many clusters are given by the model?\n",
    "\n",
    "Interpret the contingency matrix considering the answer to these questions.\n",
    "\n",
    "Ans:\n",
    "\n",
    "According to our finding, there are total 10 major clusters given by the model. Furthermore, you can find there are many categories being distributed to other categories as a cluster. \n",
    "\n",
    "It might because of the reason that density based clustering relies on having enough data to separate dense areas. \n",
    "In higher dimensional spaces this becomes more difficult, and hence requires more data. Which makes HDBSCAN hard to perform well in this high dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYRAkT7iJAT8"
   },
   "source": [
    "What does “-1” mean for the clustering labels?\n",
    "\n",
    "Ans:\n",
    "\n",
    "The “-1” in the clustering  means noise or outlier sammples that can not been classfied into a cluster by the algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I9MYIhkh6o4w",
    "outputId": "bebf24bf-b32b-40df-c17a-917d064e5a33"
   },
   "outputs": [],
   "source": [
    "# print contingency matrix\n",
    "by_pred_hdbs = hdbscan.HDBSCAN(min_cluster_size=best_mcs).fit_predict(X_umap)\n",
    "cm16 = confusion_matrix(y_a_data, by_pred_hdbs)\n",
    "rows, cols = linear_sum_assignment(cm16, maximize=True)\n",
    "plot_mat(cm16[rows[:, np.newaxis], cols], xticklabels=cols, yticklabels=rows, title='HDBSCAN with min_cluster_size=' + str(best_mcs), size=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONmAfQvR4RdS"
   },
   "source": [
    "## Question 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pb2r3mZL4ToV"
   },
   "source": [
    "Based on your experiments, which dimensionality reduction technique and clustering methods worked best together for 20-class text data and why? Follow the table below. If UMAP takes too long to converge, consider running it once and saving the intermediate results in a pickle file.\n",
    "\n",
    "|Module|Alternatives|Hyperparameters|\n",
    "|---|---|---|\n",
    "|Dimensionality Reduction|None|N/A|\n",
    "|Dimensionality Reduction|SVD|r = [5, 20, 200]|\n",
    "|Dimensionality Reduction|NMF|r = [5, 20, 200]|\n",
    "|Dimensionality Reduction|UMAP|n_components = [5, 20, 200]|\n",
    "|Clustering|K-Means|k = [10, 20, 50]|\n",
    "|Clustering|Agglomerative Clustering|n_clusters = [20]|\n",
    "|Clustering|HDBSCAN|min_cluster_size = [100, 200]|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoW_GEW3g0Em"
   },
   "source": [
    "Ans:\n",
    "\n",
    "The combination of **UMAP using 'cosine' matrix and n_components setting to 5 plus K-Means using k = 20** works best together for 20-class text data. we will analysis the reason of it in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3ohBftEWHdJ"
   },
   "outputs": [],
   "source": [
    "def pkl_save(path, data, filename):\n",
    "  with open(path + filename, 'wb') as f:\n",
    "    # compressed_file = bz2.BZ2File(f, 'w')\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfpzQ16rHU6k"
   },
   "outputs": [],
   "source": [
    "dataset_all = fetch_20newsgroups(subset = 'all', shuffle = True, random_state = 0, remove=('headers','footers'))\n",
    "vec = CountVectorizer(stop_words='english', min_df=3)\n",
    "tfidf = TfidfTransformer()\n",
    "X_vec = vec.fit_transform(dataset_all.data)\n",
    "X_tfidf = tfidf.fit_transform(X_vec)\n",
    "y_data = dataset_all.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXIG6crJHqMf"
   },
   "outputs": [],
   "source": [
    "# modify this line to your local path\n",
    "pwd = !pwd\n",
    "path = str(pwd[0]) + '/pickle file/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDDH7ETVURTW",
    "outputId": "4665299e-d60f-4a90-e8d8-36e2e30e3f4b"
   },
   "outputs": [],
   "source": [
    "# None\n",
    "pkl_save(path, X_tfidf, 'none.pkl')\n",
    "\n",
    "rs17 = [5, 20, 200]\n",
    "for i in range(len(rs17)):\n",
    "  print(\"\\ntry r =\", rs17[i], \"...\")\n",
    "  # SVD\n",
    "  print(\"using SVD ...\")\n",
    "  svd_tmp = TruncatedSVD(n_components=rs17[i], random_state=0)\n",
    "  X_svd = svd_tmp.fit_transform(X_tfidf)\n",
    "  pkl_save(path, X_svd, 'svd_' + str(rs17[i]) + '.pkl')\n",
    "\n",
    "  # NMF\n",
    "  print(\"using NMF ...\")\n",
    "  nmf_tmp = NMF(n_components=rs17[i], init='random', random_state=0)\n",
    "  X_nmf = nmf_tmp.fit_transform(X_tfidf)\n",
    "  pkl_save(path, X_nmf, 'nmf_' + str(rs17[i]) + '.pkl')\n",
    "\n",
    "  # UMAP\n",
    "  print(\"using UMAP ...\")\n",
    "  umap_tmp = umap.UMAP(n_components=rs17[i], metric='cosine')\n",
    "  X_umap = umap_tmp.fit_transform(X_tfidf)\n",
    "  pkl_save(path, X_umap, 'umap_' + str(rs17[i]) + '.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "re0Y0-KtxmeS"
   },
   "outputs": [],
   "source": [
    "def pkl_read(path, filename):\n",
    "  with open(path + filename, 'rb') as f:\n",
    "    # compressed_file = bz2.BZ2File(f, 'r')\n",
    "    X_dr = pickle.load(f)\n",
    "  return X_dr\n",
    "DRs = ['none.pkl',\n",
    "       'svd_5.pkl', 'svd_20.pkl', 'svd_200.pkl',\n",
    "       'nmf_5.pkl', 'nmf_20.pkl', 'nmf_200.pkl',\n",
    "       'umap_5.pkl', 'umap_20.pkl', 'umap_200.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXu3uyFKwwcS",
    "outputId": "be5e4b17-9c21-4ec2-ac4c-77dfff29ae93"
   },
   "outputs": [],
   "source": [
    "ks = [10, 20, 50]\n",
    "km_scores = [ [0] * len(DRs) for i in range(3)]\n",
    "for i in range(len(ks)):\n",
    "  print(\"\\nk =\", ks[i], \"...\")\n",
    "  kmeans_tmp = KMeans(n_clusters=ks[i], init='k-means++', max_iter=2000, n_init=50, random_state=0)\n",
    "  for j in range(len(DRs)):\n",
    "    print(\"Current dr:\", DRs[j])\n",
    "    # reading Dimensionality Reduction data\n",
    "    X_dr = pkl_read(path, DRs[j])\n",
    "    kmeans_fit = kmeans_tmp.fit(X_dr)\n",
    "    # To increase efficiency, we calculate the average score of each combination instead.\n",
    "    tmp_s = 0\n",
    "    tmp_s += homogeneity_score(y_data, kmeans_fit.labels_)\n",
    "    tmp_s += completeness_score(y_data, kmeans_fit.labels_)\n",
    "    tmp_s += v_measure_score(y_data, kmeans_fit.labels_)\n",
    "    tmp_s += adjusted_rand_score(y_data, kmeans_fit.labels_)\n",
    "    tmp_s += adjusted_mutual_info_score(y_data, kmeans_fit.labels_)\n",
    "    tmp_s /= 5\n",
    "    km_scores[i][j] = tmp_s\n",
    "    print(\"Kmeans when k =\", ks[i], \", average score:\", tmp_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSGTROhx_6kH",
    "outputId": "a9e433d8-3d43-48ae-9e06-a7866324364c"
   },
   "outputs": [],
   "source": [
    "agg_n = 20\n",
    "agg_scores = [ [0] * len(DRs)]\n",
    "for j in range(len(DRs)):\n",
    "  print(\"Current dr:\", DRs[j])\n",
    "  # it takes over 1hr to finish the none.pkl, we save the result that we have found before in advance here\n",
    "  if (DRs[j] == 'none.pkl'):\n",
    "    print(\"Agglomerative Clustering when n_clusters =\", agg_n, \", average score:\", 0.3437828455142916)\n",
    "    agg_scores[0][j] = 0.3437828455142916\n",
    "    continue\n",
    "  # reading Dimensionality Reduction data\n",
    "  X_dr = pkl_read(path, DRs[j])\n",
    "  # if (DRs[j] == 'none.pkl'):\n",
    "  #   X_dr = X_dr.toarray()\n",
    "  agg_tmp = AgglomerativeClustering(n_clusters=agg_n, linkage='ward').fit(X_dr)\n",
    "  # To increase efficiency, we calculate the average score of each combination instead.\n",
    "  tmp_s = 0\n",
    "  tmp_s += homogeneity_score(y_data, agg_tmp.labels_)\n",
    "  tmp_s += completeness_score(y_data, agg_tmp.labels_)\n",
    "  tmp_s += v_measure_score(y_data, agg_tmp.labels_)\n",
    "  tmp_s += adjusted_rand_score(y_data, agg_tmp.labels_)\n",
    "  tmp_s += adjusted_mutual_info_score(y_data, agg_tmp.labels_)\n",
    "  tmp_s /= 5\n",
    "  agg_scores[0][j] = tmp_s\n",
    "  print(\"Agglomerative Clustering when n_clusters =\", agg_n, \", average score:\", tmp_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LMJ7Hop5Ar51",
    "outputId": "9557295d-f4cb-4e1b-df2e-265614e3133f"
   },
   "outputs": [],
   "source": [
    "mcss = [100, 200]\n",
    "hdbs_scores = [ [0] * len(DRs) for i in range(2)]\n",
    "for i in range(len(mcss)):\n",
    "  print(\"\\nmin_cluster_size =\", mcss[i], \"...\")\n",
    "  for j in range(len(DRs)):\n",
    "    print(\"Current dr:\", DRs[j])\n",
    "    # reading Dimensionality Reduction data\n",
    "    X_dr = pkl_read(path, DRs[j])\n",
    "    y_pred_hdbs = hdbscan.HDBSCAN(min_cluster_size=mcss[i], allow_single_cluster=True).fit_predict(X_dr)\n",
    "    # To increase efficiency, we calculate the average score of each combination instead.\n",
    "    tmp_s = 0\n",
    "    tmp_s += homogeneity_score(y_data, y_pred_hdbs)\n",
    "    tmp_s += completeness_score(y_data, y_pred_hdbs)\n",
    "    tmp_s += v_measure_score(y_data, y_pred_hdbs)\n",
    "    tmp_s += adjusted_rand_score(y_data, y_pred_hdbs)\n",
    "    tmp_s += adjusted_mutual_info_score(y_data, y_pred_hdbs)\n",
    "    tmp_s /= 5\n",
    "    hdbs_scores[i][j] = tmp_s\n",
    "    print(\"HDBSCAN when min_cluster_size =\", mcss[i], \", average score:\", tmp_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KwUxN57PoSe"
   },
   "source": [
    "- It takes too long to run the gridsearch to test all possible combinations of the table. We leave the code here in case we want to utilize it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyuzH3oh6peh"
   },
   "outputs": [],
   "source": [
    "# cachedir = mkdtemp()\n",
    "# memory = Memory(cachedir, verbose=87)\n",
    "# estimators = [\n",
    "#     ('vect', CountVectorizer(stop_words='english', min_df=3)),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "#     ('reduce_dim', None),\n",
    "#     ('clf', None),\n",
    "#     ]\n",
    "# pipeline = Pipeline(estimators, memory=memory)\n",
    "# param_grid = [{\n",
    "#         'reduce_dim': [TruncatedSVD(n_components=5, random_state=0),\n",
    "#                        TruncatedSVD(n_components=20, random_state=0),\n",
    "#                        TruncatedSVD(n_components=200, random_state=0), \n",
    "#                        NMF(n_components=5, init='random', random_state=0),\n",
    "#                        NMF(n_components=20, init='random', random_state=0),\n",
    "#                        NMF(n_components=200, init='random', random_state=0),\n",
    "#                        umap.UMAP(n_components=5, metric='euclidean'),\n",
    "#                        umap.UMAP(n_components=20, metric='euclidean'),\n",
    "#                        umap.UMAP(n_components=200, metric='euclidean')],\n",
    "#         'clf': [KMeans(n_clusters=10, init='k-means++', max_iter=2000, n_init=50, random_state=0),\n",
    "#                 KMeans(n_clusters=20, init='k-means++', max_iter=2000, n_init=50, random_state=0),\n",
    "#                 KMeans(n_clusters=50, init='k-means++', max_iter=2000, n_init=50, random_state=0),\n",
    "#                 AgglomerativeClustering(n_clusters=20, linkage='ward'),\n",
    "#                 hdbscan.HDBSCAN(min_cluster_size=100),\n",
    "#                 hdbscan.HDBSCAN(min_cluster_size=200)]\n",
    "#     }]\n",
    "# CV = GridSearchCV(pipeline, cv=5, param_grid=param_grid, scoring='accuracy')\n",
    "# CV.fit(dataset_all.data, y_a_data)\n",
    "\n",
    "# for id in range(0,len(CV.cv_results_[\"rank_test_score\"])):\n",
    "#   if CV.cv_results_[\"rank_test_score\"][id] <= 5:\n",
    "#     print(CV.cv_results_[\"rank_test_score\"][id])\n",
    "#     print(CV.cv_results_[\"params\"][id])\n",
    "#     print(CV.cv_results_[\"mean_test_score\"][id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De5OTISjkadV"
   },
   "source": [
    "Ans:\n",
    "\n",
    "From the result we obtained, the combination of **UMAP using 'cosine' matrix and n_components setting to 5 plus K-Means using k = 20** works best together for 20-class text data.\n",
    "\n",
    "It might because of that the categories it need to predict is also 20. On the other hand, UMAP captures the structure quickly and efficiently comparing to other methods. Combining these two, they performs better than other combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8Lia_894cB3"
   },
   "source": [
    "## Question 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyNrmRy14imi"
   },
   "source": [
    "Extra credit: If you can find creative ways to further enhance the clustering\n",
    "performance, report your method and the results you obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "om9tXwADg0Ep"
   },
   "source": [
    "Ans:\n",
    "\n",
    "From the result we obtained, we can see that any clustering method using UMAP as the Dimensionality Reduction method performs much better than other methods.\n",
    "\n",
    "From this perspective, we try to adjust the parameters of UMAP to see if we can obtain a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Wf8-HP2kl_h",
    "outputId": "d15093e3-f727-42cb-e0db-b9469cf8763d"
   },
   "outputs": [],
   "source": [
    "rs18 = [30, 50, 100]\n",
    "for i in range(len(rs18)):\n",
    "  print(\"\\ntry r =\", rs18[i], \"...\")\n",
    "\n",
    "  # UMAP\n",
    "  print(\"using UMAP ...\")\n",
    "  umap_tmp = umap.UMAP(n_components=rs18[i], metric='cosine')\n",
    "  X_umap = umap_tmp.fit_transform(X_tfidf)\n",
    "  pkl_save(path, X_umap, 'umap_' + str(rs18[i]) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amdgexVlg0Ep"
   },
   "outputs": [],
   "source": [
    "umaps = ['umap_5.pkl', 'umap_20.pkl', 'umap_30.pkl', 'umap_50.pkl', 'umap_100.pkl', 'umap_200.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEymphJsg0Ep",
    "outputId": "4094e949-a456-4269-a264-f7e613851e26",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ks = [10, 20, 30, 50, 100]\n",
    "km_scores = [ [0] * len(umaps) for i in range(len(ks))]\n",
    "for i in range(len(ks)):\n",
    "  print(\"\\nk =\", ks[i], \"...\")\n",
    "  kmeans_tmp = KMeans(n_clusters=ks[i], init='k-means++', max_iter=2000, n_init=50, random_state=0)\n",
    "  for j in range(len(umaps)):\n",
    "    print(\"Current UMAP:\", umaps[j])\n",
    "    # reading Dimensionality Reduction data\n",
    "    X_dr = pkl_read(path, umaps[j])\n",
    "    kmeans_fit = kmeans_tmp.fit(X_dr)\n",
    "    # To increase efficiency, we calculate the average score of each combination instead.\n",
    "    tmp_s = 0\n",
    "    tmp_s += homogeneity_score(y_data, kmeans_fit.labels_)\n",
    "    tmp_s += completeness_score(y_data, kmeans_fit.labels_)\n",
    "    tmp_s += v_measure_score(y_data, kmeans_fit.labels_)\n",
    "    tmp_s += adjusted_rand_score(y_data, kmeans_fit.labels_)\n",
    "    tmp_s += adjusted_mutual_info_score(y_data, kmeans_fit.labels_)\n",
    "    tmp_s /= 5\n",
    "    km_scores[i][j] = tmp_s\n",
    "    print(\"Kmeans when k =\", ks[i], \", average score:\", tmp_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQEGDgm5g0Eq"
   },
   "source": [
    "From above result, we find that the performace of UMAP_100 is a little bit better than UMAP_50. And $k=20$ is the best setting for K-Means. We will try to adjust some parameters for these two settings to get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCoDLv29g0Eq",
    "outputId": "5e78fd7f-9325-4e05-a0ca-7ed5601e33c3"
   },
   "outputs": [],
   "source": [
    "n_neighs = [20, 30, 50, 100]\n",
    "min_dists = [0.2, 0.3, 0.5, 0.87]\n",
    "kmeans_tmp = KMeans(n_clusters=20, init='k-means++', max_iter=2000, n_init=50, random_state=0)\n",
    "for i in range(len(n_neighs)):\n",
    "    print(\"\\nn_neighbors =\", n_neighs[i], \"...\")\n",
    "    for j in range(len(min_dists)):\n",
    "        print(\"Current min_dist:\", min_dists[j])\n",
    "        umap_tmp = umap.UMAP(n_components=100, n_neighbors=n_neighs[i], min_dist=min_dists[j], metric='cosine')\n",
    "        X_umap = umap_tmp.fit_transform(X_tfidf)\n",
    "        kmeans_fit = kmeans_tmp.fit(X_umap)\n",
    "        tmp_s = 0\n",
    "        tmp_s += homogeneity_score(y_data, kmeans_fit.labels_)\n",
    "        tmp_s += completeness_score(y_data, kmeans_fit.labels_)\n",
    "        tmp_s += v_measure_score(y_data, kmeans_fit.labels_)\n",
    "        tmp_s += adjusted_rand_score(y_data, kmeans_fit.labels_)\n",
    "        tmp_s += adjusted_mutual_info_score(y_data, kmeans_fit.labels_)\n",
    "        tmp_s /= 5\n",
    "        km_scores[i][j] = tmp_s\n",
    "        print(\"Kmeans when k = 20\", \", UMAP when n_neighbors =\", n_neighs[i], \", min_dist =\", min_dists[j], \", average score:\", tmp_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQxv5hxRg0Eq"
   },
   "source": [
    "After the experiment, we found that the performance usually gets better when we increase the **min_dist** parameter. Which makes sense because as the minimum distance between each data point increase, we should separate the cluster more easily and get a higher Homogeneity and Completeness score.\n",
    "\n",
    "In conclusion, we found a better clustering performance by using **UMAP with $ n\\_components = 100, n\\_neighbors = 50$, and $min\\_dist = 0.87$. Combining it with K-means with k = 20**, we get a average score which is approximately $0.597$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRPFvwF14klY"
   },
   "source": [
    "## Question 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nae4rZV_4mfB"
   },
   "source": [
    "In a brief paragraph discuss: If the VGG network is trained on a dataset with\n",
    "perhaps totally different classes as targets, why would one expect the features derived from such a network to have discriminative power for a custom dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNR8PKVm6qru"
   },
   "source": [
    "Ans:\n",
    "\n",
    "We expect that the model is trained on a large and general enough dataset. Even though it's a dataset with perhaps totally different classes as targets, we can still get advantage from its learned feature maps to repurpose it. \n",
    "\n",
    "To be specifically, there are always multiple layers inside our network model. We can imagine that the final layer of our network is the one that learn to identify classes specific to our project that need training. The initial layers or the middle layers are used to detect slant lines no matter what you want to classify. Therefore, we can always reuse the neural network instead of retraining them every time we create a new one.\n",
    "\n",
    "Finally, after getting the pretrained model, we can use fine-tuning to make the model more relevant to the custom dataset we want to use. By doing this, we can expect that the features derived from previous network will have discriminative power for a custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0LoF9SH4rHe"
   },
   "source": [
    "## Question 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atEHjqvI4vLx"
   },
   "source": [
    "In a brief paragraph explain how the helper code base is performing feature\n",
    "extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXulLP_V6rSZ"
   },
   "source": [
    "Ans:\n",
    "\n",
    "It first extracts feature layers and pooling layer from the VGG-16. After that, it constructs a flatten layer using torch, and extracts the first part of fully-connected layer from VGG-16.\n",
    "\n",
    "Similar to the concept we mentioned in Q19, it extracts the basic layers from the pretrained model VGG-16. By doing this, it extracts the basic discriminative pwoer we have in the pretrained model. We can use it in further application such as fine-tuning on the custom dataset we want to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANGbU6dE4yd5"
   },
   "source": [
    "## Question 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191,
     "referenced_widgets": [
      "0da83961cf7049f5bd97f9b75abcdb51",
      "ec052ba44a9f45af898041cc0d568ee0",
      "e1aa0b840e7444a9a031e16a206b45a2",
      "ed1f2f29b05545cf8a6e76cbc5df8dda",
      "ca5679ab3476417b97c3ab17a7b2c60b",
      "44e70be9b3cc4e49bad13efc74a426f7",
      "1e08591143b84e55992b1db15287b08c",
      "343261a6b3974cce934583fe28fef651",
      "8386befbcf244199b69a006f93df6aed",
      "58c8e8f24b0540e796bbd0c137df71ee",
      "c2e74c7877ee45579a0dc4794642c5e0"
     ]
    },
    "id": "GSo0-46kcc5q",
    "outputId": "409a6677-9cb2-4074-8fae-786a6cb35386"
   },
   "outputs": [],
   "source": [
    "filename = './flowers_features_and_labels.npz'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    file = np.load(filename)\n",
    "    f_all, y_all = file['f_all'], file['y_all']\n",
    "\n",
    "else:\n",
    "    if not os.path.exists('./flower_photos'):\n",
    "        # download the flowers dataset and extract its images\n",
    "        url = 'http://download.tensorflow.org/example_images/flower_photos.tgz'\n",
    "        with open('./flower_photos.tgz', 'wb') as file:\n",
    "            file.write(requests.get(url).content)\n",
    "        with tarfile.open('./flower_photos.tgz') as file:\n",
    "            file.extractall('./')\n",
    "        os.remove('./flower_photos.tgz')\n",
    "\n",
    "    class FeatureExtractor(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\n",
    "\n",
    "            # Extract VGG-16 Feature Layers\n",
    "            self.features = list(vgg.features)\n",
    "            self.features = nn.Sequential(*self.features)\n",
    "            # Extract VGG-16 Average Pooling Layer\n",
    "            self.pooling = vgg.avgpool\n",
    "            # Convert the image into one-dimensional vector\n",
    "            self.flatten = nn.Flatten()\n",
    "            # Extract the first part of fully-connected layer from VGG16\n",
    "            self.fc = vgg.classifier[0]\n",
    "\n",
    "        def forward(self, x):\n",
    "            # It will take the input 'x' until it returns the feature vector called 'out'\n",
    "            out = self.features(x)\n",
    "            out = self.pooling(out)\n",
    "            out = self.flatten(out)\n",
    "            out = self.fc(out) \n",
    "            return out \n",
    "\n",
    "    # Initialize the model\n",
    "    assert torch.cuda.is_available()\n",
    "    feature_extractor = FeatureExtractor().cuda().eval()\n",
    "\n",
    "    dataset = datasets.ImageFolder(root='./flower_photos',\n",
    "                                   transform=transforms.Compose([transforms.Resize(224),\n",
    "                                                                 transforms.CenterCrop(224),\n",
    "                                                                 transforms.ToTensor(),\n",
    "                                                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Extract features and store them on disk\n",
    "    f_all, y_all = np.zeros((0, 4096)), np.zeros((0,))\n",
    "    for x, y in tqdm(dataloader):\n",
    "        with torch.no_grad():\n",
    "            f_all = np.vstack([f_all, feature_extractor(x.cuda()).cpu()])\n",
    "            y_all = np.concatenate([y_all, y])\n",
    "    np.savez(filename, f_all=f_all, y_all=y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlaSxdr-43cR"
   },
   "source": [
    "How many pixels are there in the original images? How many features does\n",
    "the VGG network extract per image; i.e what is the dimension of each feature vector for an image sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlqMGCXLjxUX"
   },
   "source": [
    "Ans:\n",
    "\n",
    "There are 224 x 224 pixels in the original images.\n",
    "\n",
    "There are totally 3670 features being extracted by the VGG network per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Njt3S-zGeI20",
    "outputId": "d9f8163b-6f17-465e-bb8d-7de9bdb69e5b"
   },
   "outputs": [],
   "source": [
    "for x, y in tqdm(dataloader):\n",
    "  print(\"\\n\")\n",
    "  print(x.size())\n",
    "  print(y.size())\n",
    "  break\n",
    "\n",
    "print(\"\\nfeature vector:\", f_all.shape, y_all.shape)\n",
    "num_features = f_all.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NbLVjRi45fA"
   },
   "source": [
    "## Question 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4myw7neD4-lV"
   },
   "source": [
    "Are the extracted features dense or sparse? (Compare with sparse TF-IDF\n",
    "features in text.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZ8tQg2r6sRA"
   },
   "source": [
    "Ans:\n",
    "\n",
    "From Q10, we found the shape of TFIDX dataset: (18846, 45365)\n",
    "\n",
    "The extracted feature vectors we found: (3670, 4096)\n",
    "\n",
    "Furthermore, we can see that the numbers inside TF-IDF features are much closer to zero comparing to the extracted features. Therefore, the extracted features are more dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2P0Y0OPaMgu5",
    "outputId": "ef75a33d-238a-45b4-fb8d-00b6ac51ebfb"
   },
   "outputs": [],
   "source": [
    "print(f_all.shape)\n",
    "print(f_all)\n",
    "print(X_tfidf.shape)\n",
    "print(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVjm6HIL4_qd"
   },
   "source": [
    "## Question 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9Y5x-op5EXM"
   },
   "source": [
    "In order to inspect the high-dimensional features, t-SNE is a popular off-the-shelf choice for visualizing Vision features. Map the features you have extracted onto 2 dimensions with t-SNE. Then plot the mapped feature vectors along $x$ and $y$ axes. Color-code the data points with ground-truth labels. Describe your observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYRA2xEaMdai"
   },
   "source": [
    "Ans:\n",
    "\n",
    "From our result, we can see that there are 5 categories (clusterings) and they have many overlapping spots when we project it onto 2 dimensions.\n",
    "\n",
    "It indicates that we might face some problems when we try to use high dimensional clustering method on this dataset. The distance between each data point is really close according to our projection result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "AbzFWpCQwKMU",
    "outputId": "3296d8a0-75a2-4cba-e182-437304fac65b"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "z = tsne.fit_transform(f_all)\n",
    "df = pd.DataFrame()\n",
    "df[\"y\"] = y_all\n",
    "df[\"dim_1\"] = z[:,0]\n",
    "df[\"dim_2\"] = z[:,1]\n",
    "sns.scatterplot(x=\"dim_1\", y=\"dim_2\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", as_cmap = True),\n",
    "                data=df).set(title=\"Extracted features maapped onto 2-D with T-SNE projection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_xiBtkZ5LGQ"
   },
   "source": [
    "## Question 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqhuGYku6swl"
   },
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrjjqJRYIyta"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module, TransformerMixin):\n",
    "    def __init__(self, n_components):\n",
    "        super().__init__()\n",
    "        self.n_components = n_components\n",
    "        self.n_features = None  # to be determined with data\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        \n",
    "    def _create_encoder(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(4096, 1280),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1280, 640),\n",
    "            nn.ReLU(True), nn.Linear(640, 120), nn.ReLU(True), nn.Linear(120, self.n_components))\n",
    "    \n",
    "    def _create_decoder(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.n_components, 120),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(120, 640),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(640, 1280),\n",
    "            nn.ReLU(True), nn.Linear(1280, 4096))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        encoded = self.encoder(X)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def fit(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device='cuda')\n",
    "        self.n_features = X.shape[1]\n",
    "        self.encoder = self._create_encoder()\n",
    "        self.decoder = self._create_decoder()\n",
    "        self.cuda()\n",
    "        self.train()\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "        dataset = TensorDataset(X)\n",
    "        dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        for epoch in tqdm(range(100)):\n",
    "            for (X_,) in dataloader:\n",
    "                X_ = X_.cuda()\n",
    "                # ===================forward=====================\n",
    "                output = self(X_)\n",
    "                loss = criterion(output, X_)\n",
    "                # ===================backward====================\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        return self     \n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device='cuda')\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.encoder(X).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rnR1_Jf5Q5T"
   },
   "source": [
    "Report the best result (in terms of rand score) within the table below.\n",
    "For HDBSCAN, introduce a conservative parameter grid over min cluster size and min samples.\n",
    "\n",
    "|Module|Alternatives|Hyperparameters|\n",
    "|---|---|---|\n",
    "|Dimensionality Reduction|None|N/A|\n",
    "|Dimensionality Reduction|SVD|r = 50|\n",
    "|Dimensionality Reduction|UMAP|n_components = 50|\n",
    "|Dimensionality Reduction|Autoencoder|num_features = 50|\n",
    "|Clustering|K-Means|k = 5|\n",
    "|Clustering|Agglomerative Clustering|n_clusters = 5|\n",
    "|Clustering|HDBSCAN|min_cluster_size & min_samples|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6grmnfhD_7tL"
   },
   "source": [
    "Ans:\n",
    "\n",
    "The best rand score we get is approximately 0.467. Which is derived from the combination of UMAP [n_components = 50] + K-Means [k = 5].\n",
    "\n",
    "For HDBSCAN, we set min_cluster_size = [50, 100, 200], min_samples = [20, 50, 100, 200] to proceed a grid search. However, the best rand score we can get is approximately only 0.094 when combining with UMAP [n_components = 50]. Which is still much worse than other combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eU51IfAbIms5"
   },
   "outputs": [],
   "source": [
    "# modify this line to your local path\n",
    "pwd = !pwd\n",
    "path = str(pwd[0]) + '/pickle file/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAheYaXu6tkx",
    "outputId": "7757bee9-a26d-412e-c121-d7f25ede96d9"
   },
   "outputs": [],
   "source": [
    "# None\n",
    "pkl_save(path, f_all, 'f_none.pkl')\n",
    "\n",
    "r24 = 50\n",
    "print(\"\\ntry r =\", r24, \"...\")\n",
    "# SVD\n",
    "print(\"using SVD ...\")\n",
    "svd_tmp = TruncatedSVD(n_components=r24, random_state=0)\n",
    "X_svd = svd_tmp.fit_transform(f_all)\n",
    "pkl_save(path, X_svd, 'f_svd_' + str(r24) + '.pkl')\n",
    "\n",
    "# UMAP\n",
    "print(\"using UMAP ...\")\n",
    "umap_tmp = umap.UMAP(n_components=r24, metric='cosine')\n",
    "X_umap = umap_tmp.fit_transform(f_all)\n",
    "pkl_save(path, X_umap, 'f_umap_' + str(r24) + '.pkl')\n",
    "\n",
    "# Autoencoder\n",
    "print(\"using Autoencoder ...\")\n",
    "X_aec = Autoencoder(r24).fit_transform(f_all)\n",
    "pkl_save(path, X_aec, 'f_aec_' + str(r24) + '.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hogMBK42KB0u"
   },
   "outputs": [],
   "source": [
    "DRs = ['f_none.pkl', 'f_svd_50.pkl', 'f_umap_50.pkl', 'f_aec_50.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yb3yn3e5MG0E",
    "outputId": "0e608789-fc2e-4fc0-9ddd-41e9a30fddb4"
   },
   "outputs": [],
   "source": [
    "ks = [5]\n",
    "km_scores24 = [ [0] * len(DRs) for i in range(len(ks))]\n",
    "for i in range(len(ks)):\n",
    "  print(\"\\nk =\", ks[i], \"...\")\n",
    "  kmeans_tmp = KMeans(n_clusters=ks[i], init='k-means++', max_iter=2000, n_init=50, random_state=0)\n",
    "  for j in range(len(DRs)):\n",
    "    print(\"Current dr:\", DRs[j])\n",
    "    # reading Dimensionality Reduction data\n",
    "    X_dr = pkl_read(path, DRs[j])\n",
    "    kmeans_fit = kmeans_tmp.fit(X_dr)\n",
    "    # As the question mentioned, we use rand score to determine which combination performs best.\n",
    "    tmp_s = adjusted_rand_score(y_all, kmeans_fit.labels_)\n",
    "    km_scores24[i][j] = tmp_s\n",
    "    print(\"Kmeans when k =\", ks[i], \", rand score:\", tmp_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MvzhUJPxMXZW",
    "outputId": "1994d5db-7408-4829-ec3a-f0069d77b185"
   },
   "outputs": [],
   "source": [
    "agg_n = 5\n",
    "agg_scores24 = [ [0] * len(DRs)]\n",
    "for j in range(len(DRs)):\n",
    "  print(\"Current dr:\", DRs[j])\n",
    "  # reading Dimensionality Reduction data\n",
    "  X_dr = pkl_read(path, DRs[j])\n",
    "  agg_tmp = AgglomerativeClustering(n_clusters=agg_n, linkage='ward').fit(X_dr)\n",
    "  # As the question mentioned, we use rand score to determine which combination performs best.\n",
    "  tmp_s = adjusted_rand_score(y_all, agg_tmp.labels_)\n",
    "  agg_scores24[0][j] = tmp_s\n",
    "  print(\"Agglomerative Clustering when n_clusters =\", agg_n, \", rand score:\", tmp_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yj1X8AeZM6hW",
    "outputId": "a709c1b4-62c2-4a5f-d7de-09a58d861a10"
   },
   "outputs": [],
   "source": [
    "mc_ss = [50, 100, 200]\n",
    "min_ss = [20, 50, 100, 200]\n",
    "# hdbs_scores24 = [ [0] * len(DRs) for i in range(2)]\n",
    "for i in range(len(mc_ss)):\n",
    "  print(\"\\nmin_cluster_size =\", mc_ss[i], \"...\")\n",
    "  for k in range(len(min_ss)):\n",
    "    print(\"\\nmin_samples =\", min_ss[k], \"...\")\n",
    "    for j in range(len(DRs)):\n",
    "      print(\"Current dr:\", DRs[j])\n",
    "      # reading Dimensionality Reduction data\n",
    "      X_dr = pkl_read(path, DRs[j])\n",
    "      y_pred_hdbs = hdbscan.HDBSCAN(min_cluster_size=mc_ss[i], min_samples=min_ss[k], allow_single_cluster=True).fit_predict(X_dr)\n",
    "      # As the question mentioned, we use rand score to determine which combination performs best.\n",
    "      tmp_s = adjusted_rand_score(y_all, y_pred_hdbs)\n",
    "      # hdbs_scores24[i][j] = tmp_s\n",
    "      print(\"HDBSCAN when min_cluster_size =\", mc_ss[i], \", min_samples =\", min_ss[k], \", rand score:\", tmp_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5u6TQsX6P_K"
   },
   "source": [
    "## Question 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgbnNKRbQn6P"
   },
   "source": [
    "### MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sc7ICMvQtqM"
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_features, 1280),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1280, 640),\n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(640, 5),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        self.cuda()\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device='cuda')\n",
    "        y = torch.tensor(y, dtype=torch.int64, device='cuda')\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "        dataset = TensorDataset(X, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        for epoch in tqdm(range(100)):\n",
    "            for (X_, y_) in dataloader:\n",
    "                X_ = X_.cuda()\n",
    "                y_ = y_.cuda()\n",
    "                # ===================forward=====================\n",
    "                output = self(X_)\n",
    "                loss = criterion(output, y_)\n",
    "                # ===================backward====================\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "    \n",
    "    def eval(self, X_test, y_test):\n",
    "        X = torch.tensor(X_test, dtype=torch.float32, device='cuda')\n",
    "        y = torch.tensor(y_test, dtype=torch.int64, device='cuda')\n",
    "        dataset = TensorDataset(X, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=128, shuffle = False)\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        for (X_, y_) in dataloader:\n",
    "            X_ = X_.cuda()\n",
    "            y_ = y_.cuda()\n",
    "            logits = self(X_)\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "            num_examples += y_.size(0)\n",
    "            correct_pred += (predicted_labels == y_).sum()\n",
    "            # print(\"predicted_labels\", predicted_labels)\n",
    "            # print(\"y\", y_)\n",
    "        acc = correct_pred.float() / num_examples * 100\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6_OvdZd6jXU"
   },
   "source": [
    "Report the test accuracy of the MLP classifier on the original VGG features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-ct_2Yu6mlw",
    "outputId": "0959d980-ead5-4161-90b5-561bfd340219"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(f_all, y_all, test_size=0.2, random_state=0)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ExOWIdWeJWT",
    "outputId": "f7bc313c-9843-4ba1-827d-e43caaf5bdce"
   },
   "outputs": [],
   "source": [
    "model = MLP(X_train.shape[1])\n",
    "model.train(X_train, y_train)\n",
    "acc = model.eval(X_test, y_test)\n",
    "print(\"\\nMLP classifier on the original VGG features\")\n",
    "print(\"Test accuracy: %.2f\" % acc, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSxMHUiY9AFl"
   },
   "source": [
    "Report the same when using the reduced-dimension features (you have freedom in choosing the dimensionality reduction algorithm and its parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fRAytvtme7f",
    "outputId": "61dce437-e7c2-405c-a442-d9cb41afd897"
   },
   "outputs": [],
   "source": [
    "umap_50 = umap.UMAP(n_components=50, metric='cosine')\n",
    "X_umap = umap_50.fit_transform(f_all)\n",
    "# X_umap = pkl_read(path, 'f_umap_50.pkl')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_umap, y_all, test_size=0.2, random_state=0)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJXMXs_G7Rs6",
    "outputId": "fed5af96-7ab4-4048-c079-85fd983562a2"
   },
   "outputs": [],
   "source": [
    "model = MLP(X_train.shape[1])\n",
    "model.train(X_train, y_train)\n",
    "acc = model.eval(X_test, y_test)\n",
    "print(\"\\nMLP classifier on the reduced-dimension features\")\n",
    "print(\"Test accuracy: %.2f\" % acc, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkLVGU2N8Yuy"
   },
   "source": [
    "Does the performance of the model suffer with the reduced-dimension representations? Is it significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPYqTFU09FWC"
   },
   "source": [
    "Ans:\n",
    "\n",
    "Yes, the performance of the model gets worse when we use the reduced-dimension representation. It is not significant, it only decreases the test accuracy by 4% approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcZhY1H_9jRh"
   },
   "source": [
    "Does the success in classification make sense in the context of the clustering results obtained for the same features in Question 24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPThgEsj9nrR"
   },
   "source": [
    "Ans:\n",
    "\n",
    "Yes, it makes sense. The performance of the model should not be affected a lot if it's already a well-clustering dataset. Even though we reduce the dimensions of the features, the model can still get the structure of the data and performs well eventually."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0da83961cf7049f5bd97f9b75abcdb51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec052ba44a9f45af898041cc0d568ee0",
       "IPY_MODEL_e1aa0b840e7444a9a031e16a206b45a2",
       "IPY_MODEL_ed1f2f29b05545cf8a6e76cbc5df8dda"
      ],
      "layout": "IPY_MODEL_ca5679ab3476417b97c3ab17a7b2c60b"
     }
    },
    "1e08591143b84e55992b1db15287b08c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "343261a6b3974cce934583fe28fef651": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44e70be9b3cc4e49bad13efc74a426f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58c8e8f24b0540e796bbd0c137df71ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8386befbcf244199b69a006f93df6aed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c2e74c7877ee45579a0dc4794642c5e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca5679ab3476417b97c3ab17a7b2c60b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1aa0b840e7444a9a031e16a206b45a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_343261a6b3974cce934583fe28fef651",
      "max": 553433881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8386befbcf244199b69a006f93df6aed",
      "value": 553433881
     }
    },
    "ec052ba44a9f45af898041cc0d568ee0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44e70be9b3cc4e49bad13efc74a426f7",
      "placeholder": "​",
      "style": "IPY_MODEL_1e08591143b84e55992b1db15287b08c",
      "value": "100%"
     }
    },
    "ed1f2f29b05545cf8a6e76cbc5df8dda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58c8e8f24b0540e796bbd0c137df71ee",
      "placeholder": "​",
      "style": "IPY_MODEL_c2e74c7877ee45579a0dc4794642c5e0",
      "value": " 528M/528M [00:02&lt;00:00, 265MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
